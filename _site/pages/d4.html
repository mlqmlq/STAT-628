
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Discussion 4</title>
    <meta name="author" content="Linquan Ma">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="http://mlqmlq.github.io/STAT628/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">
    <link href="http://mlqmlq.github.io/STAT628/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
    <link href="http://mlqmlq.github.io/STAT628/assets/themes/twitter/css/main.css" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->

    <!-- atom & rss feed -->
    <link href="nil" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="nil" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">

  </head>

  <body>
    <div class="navbar">
      <div class="navbar-inner">
        <div class="container-narrow">
          <a class="brand" href="/">Data Science Practicum - STAT 628 - Fall 2020</a>
          <ul class="nav">
              <li><a href="/pages/schedule.html">schedule</a></li>
          </ul>

        </div>
      </div>
    </div>

    <div class="container-narrow">

      <div class="content">
        
<div class="page-header">
  <h2>Discussion 4 </h2>
</div>

<div class="row-fluid">
  <div class="span12">
    <h2 id="grading-policy-for-module-1-coding-portion">Grading policy for Module 1 (coding portion)</h2>

<p>Loss function:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nf">return</span><span class="p">((</span><span class="n">t</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
  </span><span class="p">}</span><span class="w"> 
  </span><span class="k">if</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">x</span><span class="m">+10</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="m">1000</span><span class="p">)</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">x</span><span class="m">+20</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="m">5000</span><span class="p">)</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">x</span><span class="m">+30</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="m">10000</span><span class="p">)</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">x</span><span class="m">+40</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="m">20000</span><span class="p">)</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">x</span><span class="m">+50</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="m">40000</span><span class="p">)</span><span class="w">
  </span><span class="nf">return</span><span class="p">((</span><span class="n">x</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>How did we generate the 9-dim vector in the report:</p>

<p>The first three elements indicates whether your code throws error for the three dataset we use. If not, you get 1. Otherwise, you get 0. If your code doesn’t throw error for any of them, at least you will get 2 points for the coding portion.</p>

<p>The fourth through sixth elements are generated by</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span><span class="p">(</span><span class="n">your_function</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">maxcap</span><span class="p">),</span><span class="w"> </span><span class="n">trueTime</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>The last three elements represents the running time of your code, generated by the following code:</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">codetime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">start</span><span class="o">=</span><span class="n">Sys.time</span><span class="p">()</span><span class="w">
  </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">your_function</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">maxcap</span><span class="p">)</span><span class="w"> 
  </span><span class="n">end</span><span class="o">=</span><span class="n">Sys.time</span><span class="p">()</span><span class="w">
  </span><span class="n">codetime</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">as.numeric</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>You will get more points for  lower loss ($\le 10000$), quick running time (finishes within $10^{-3}s$) and good-looking scalability plot.</p>

<p>Also, the score for the coding portion only reflects how well your predictions are on those three <strong>specific</strong> datasets. Don’t be upset if you didn’t get a satisfied score, because</p>

<ul>
  <li>The second and thrid worth much more points than the first one</li>
  <li>The grade distribution of this course based on the history is generous (nearly all the students got an A)</li>
</ul>

<p>If you have any concerns for the grade of the coding portion, feel free to talk with me during my office hours.</p>

<h2 id="gradient-descent">Gradient descent</h2>

<p>Most of the time, when training a machine learning model, we need to perform continuous nonlinear opttimization over an objective function. In this discussion, I will cover the gradient descent algorithm and its property.</p>

<p>Throughout the discussion, we consider the unconstrained minimization of a smooth convex function:
\[\min_{x\in\mathbb R^n} f(x)\]
Most of the following materials are from <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=VbHYoSyelFcC&amp;oi=fnd&amp;pg=PR17&amp;dq=nocedal+and+wright&amp;ots=32L8zqCaWL&amp;sig=soXZhH9lxsbCS4W9U7u4iMRX_Xc#v=onepage&amp;q=nocedal%20and%20wright&amp;f=false">Nocedal and Wright (2006)</a>.</p>
<h3 id="foundations">Foundations</h3>

<h4 id="taylors-theorem">Taylor’s Theorem</h4>
<p><strong>Theorem 1.</strong> Given a continuously differentiable function $f : \mathbb R^n\rightarrow \mathbb R$, and given $x, p \in \mathbb R^n$, we have 
\[f(x+p) = f(x) + \int_0^1\nabla f(x+\gamma p)^Tpd\gamma \]
\[f(x+p) = f(x) + \nabla f(x+\gamma p)^Tp, \hspace{5mm} \text{some }\gamma\in[0,1] \]</p>

<p>We sometimes call the first equation the “integral form” and the second the “mean-value form” of Taylor’s theorem.</p>

<p>A crucial quantity in optimization is the Lipschitz constant $L$ for the gradient of $f$, which is defined to satisfy</p>

<p><strong>Definition 1.</strong> A continuously differentiable function $f$ is called <em>L-smooth</em> or has <em>L-Lipschitz gradients</em> if 
\[||\nabla f(x) - \nabla f(y)||\le L||x-y|| \]</p>

<h3 id="descent-direction">Descent direction</h3>
<p><strong>Definition 2.</strong> $d$ is a <em>descent direction</em> for $f$ at $x$ if $f(x + td) &lt; f(x)$ for all $t &gt; 0$ sufficiently small.</p>

<p><strong>Proposition 1.</strong> If $f$ is continuously differentiable in a neighborhood of $x$, then any $d$ such that $d^T \nabla f(x) &lt; 0$ is a descent direction.</p>

<p><strong>Proof:</strong> By continuity of $\nabla f$, we can identify $\bar t &gt; 0$ such that $\nabla f(x + td)^Td &lt; 0$ for all $t \in [0, \bar t]$. Hence, for any $t\in [0, \bar t]$, by the “mean-value form” of Taylor’s theorem, we have 
\[f(x+td) = f(x) + t\nabla f(x+t\gamma d)^Td \hspace{5mm} \text{for some }\gamma\in[0,1]\]
Since $t\gamma\in[0, \bar t]$, we have $\nabla f(x+t\gamma d)^Td &lt; 0$. Therefore, $f(x+td) &lt; f(x)$ for $t$ sufficiently small, which indicates $d$ is a descent direction. $\blacksquare$</p>

<p>Note that among all directions with unit norm,
\[\inf_{||d||=1}d^T\nabla f(x) = -\nabla f(x) \hspace{5mm} \text{achieved when } d = -\dfrac{\nabla f(x)}{||\nabla f(x)||}.\]</p>

<p>For this reason, we refer to $-\nabla f(x)$ as the direction of <em>steepest descent</em>.</p>

<p>Since this direction always provides a descent direction, the simplest method for optimization of a smooth function has the iterations
\[x_{k+1} = x_k - \alpha_k\nabla f(x). \]
If $f$ is convex, we will get a global minimizer of $f$. This algorithm is called the <em>the method of steepest descent</em>. We will then analyze how many iterations are required to ﬁnd points where the gradient nearly vanishes.</p>

<div style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/1280px-Gradient_descent.svg.png" width="300" /></div>

<p>The simplest protocol is to set $\alpha_k \equiv \alpha$ and simply iterate as follows:
\[x_{k+1} = x_k - \alpha\nabla f(x).\]
There is a need to choose the correct $\alpha$ to make the algorithm convergent efficiently. For example, if we choose $\alpha$ too big, the following situation may happen:</p>
<div style="text-align:center"><img src="https://i.stack.imgur.com/0tirm.png" width="300" /></div>

<p>Different assumptions will lead to difference convergence rates. We will investigate them one by one.</p>

<h3 id="properties-of-steepest-descent">Properties of steepest descent</h3>

<p><strong>1. General case (L-smooth):</strong></p>

<p>Using the integral form of the Taylor Theorem, by setting $p = \alpha d$, we have 
\begin{equation}
  \begin{aligned}
    f(x+\alpha d) &amp;= f(x) + \alpha\nabla f(x)^Td+\alpha\int_0^1[\nabla f(x+\gamma\alpha d) - \nabla f(x)]^Td \text{ d} \gamma\newline
    &amp;\le f(x) + \alpha\nabla f(x)^Td+\alpha\int_0^1||\nabla f(x+\gamma\alpha d) - \nabla f(x)||\cdot||d||\text{ d}\gamma\newline
    &amp;\le f(x) + \alpha\nabla f(x)^Td+\alpha\int_0^1L||\gamma\alpha d||\cdot||d||\text{ d}\gamma\newline
    &amp;= f(x) + \alpha\nabla f(x)^Td+\alpha^2L||d||^2\int_0^1\gamma\text{ d}\gamma\newline
    &amp;= f(x) + \alpha\nabla f(x)^Td+\alpha^2\dfrac{L}{2}||d||^2.
  \end{aligned}
\end{equation}</p>

<p>For $x = x_k$ and $d = −\nabla f(x_k)$, the value of $\alpha$ that minimizes the expression on the right-hand side is $\alpha = 1/L$. By substituting these values, we obtain
\[f(x_{k+1}) = f(x_k - \nabla f(x_k)/L)\le f(x_k) - \dfrac{1}{2L}||\nabla f(x_k)||^2.  \]
Therefore, for the $T$-th iteration, we have 
\[f(x_T) \le f(x_0) - \dfrac{1}{2L}\sum_{k = 0}^{T-1}||\nabla f(x_k)||^2. \]<br />
By assuming $f(x) \ge \bar f$ for all $x$, we have 
\[\sum_{k = 0}^{T-1}||\nabla f(x_k)||^2 \le 2L[f(x_0) - \bar f] \]
Notice that \[ \min_{0\le k\le T-1}||\nabla f(x_k)||^2\le \dfrac{1}{T}\sum_{k=0}^{T-1}||\nabla f(x_k)||^2, \]
we have 
\[\min_{0\le k\le T-1}||\nabla f(x_k)||\le\sqrt{\dfrac{2L[f(x_0) - \bar f]}{T}} \]
Thus, we have shown that after $T$ steps of steepest descent, we can ﬁnd a point $x$ satisfying
\[||\nabla f(x_k)|| \le \sqrt{\dfrac{2L[f(x_0) - \bar f]}{T}}\]</p>

<p><strong>2. Convex case:</strong></p>

<p><strong>Theorem 2.</strong> Suppose that $f$ is convex and $L$-smooth, and $x^* $ is a global minimizer of $f$. Then the steepest descent method with stepsize $\alpha \equiv 1/L$ generates a sequence {$x_k$}  that satisfies 
\[f(x_T) - f(x^* ) \le \dfrac{L}{2T}||x_0 - x^* ||^2, \text{ for } T = 1,2,…\]
You can try to prove this theorem by using the inequality $f(x^* ) \ge f(x_k) + \nabla f(x^k)^T(x^* - x_k)$.</p>

<p><strong>3. Strongly convex case:</strong></p>

<p><strong>Definition 3.</strong> The smooth function $f: \mathbb R^d\rightarrow \mathbb R$ is strongly convex with modulus $m$ if there is a scalar $m &gt; 0$ such that
\[f(z)\geq f(x) + \nabla f(x)^T(z-x) + \dfrac{m}{2}||z-x||^2. \]</p>
<ul>
  <li>If $f$ is a $L$-smooth function, then 
\[f_\mu(x) = f(x) + \mu||x||^2\]
is strongly convex for $\mu$ large enough.</li>
</ul>

<p><strong>Theorem 3.</strong> Suppose that $f$ is strongly convex with modulus $m$ and $L$-smooth, and $x^* $ is a global minimizer of $f$. Then the steepest descent method with stepsize $\alpha \equiv 1/L$ generates a sequence {$x_k$}  that satisfies 
\[f(x_T) - f(x^* ) \le (1-\dfrac{m}{L})^T||f(x_0) - f(x^* ) ||^2, \text{ for } T = 1,2,…\]</p>

<p><strong>4. Comparison between rates:</strong></p>

<p>For the general case where we only assume $L$-smoothness, an iteration $k$ can be found such that $||\nabla f(x_k)||\leq \epsilon$ when
\[k\geq \dfrac{2L[f(x_0) - \bar f]}{\epsilon^2}.\]</p>

<p>For the weakly convex case, we have $f(x_k) - f(x^* )\leq \epsilon$ when 
\[k\geq \dfrac{L||x_0 - x^* ||^2}{2\epsilon}.\]</p>

<p>For the strongly convex case, we have $f(x_k) - f(x^* )\leq \epsilon$ when 
\[k\geq \dfrac{L}{m}\log\dfrac{f(x_0) - f(x^* )}{\epsilon}\]</p>

<p>When $\epsilon$ is small (for example $\epsilon&lt;10^{-6}$), the third case would convergence dramatically faster.</p>

<h2 id="exercise">Exercise:</h2>

<p>Write a code to apply various ﬁrst-order methods to the convex quadratic function $f(x) = (1/2)x^TAx$, where the positive definite matrix $A$ (of dimension $100\times 100$) is generate by the following code to have eigenvalues randomly distributed in a range of $[m, L]$, with $0&lt; m &lt; L$:</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="n">mu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">;</span><span class="w"> </span><span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">;</span><span class="w"> </span><span class="n">kappa</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">L</span><span class="o">/</span><span class="n">mu</span><span class="w">
</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">);</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="o">^</span><span class="n">D</span><span class="p">;</span><span class="w"> </span><span class="n">Dmin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">D</span><span class="p">);</span><span class="w"> </span><span class="n">Dmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">D</span><span class="p">)</span><span class="w">
</span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">D</span><span class="o">-</span><span class="n">Dmin</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">Dmax</span><span class="o">-</span><span class="n">Dmin</span><span class="p">)</span><span class="w">
</span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mu</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">D</span><span class="o">*</span><span class="p">(</span><span class="n">L</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="w">
</span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">D</span><span class="p">)</span><span class="w"> 
</span><span class="n">x0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">x_star</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>It’s obvious that $x^* $ is obviously a global minimizer of $f$. In all cases, start from a point $x_0$ generated by the <code class="language-plaintext highlighter-rouge">R</code> command <code class="language-plaintext highlighter-rouge">runif(n)</code> and run until $f(x_k) - f(x^* ) \le 10^{-6}$.</p>

<p>Implement the following methods:</p>
<ul>
  <li>Steepest descent with $\alpha_k \equiv 1/L$, that is,
\[x_{k+1} = x_{k} - (1/L)\nabla f(x_k).\]</li>
  <li>Steepest descent with $\alpha_k \equiv 1/(5L)$.</li>
  <li><a href="http://papers.nips.cc/paper/5322-a-differential-equation-for-modeling-nesterovs-accelerated-gradient-method-theory-and-insights.pdf">Nesterov’s accelerated gradient descent</a>, with the following updates:
\[y_k = x_k + \beta_k(x_k - x_{k-1}),\]
\[x_{k+1} = y^k - \alpha_k\nabla f(y_k),\]
with optim choice of $\alpha_k = 1/L$ and $\beta_k = (\sqrt L - \sqrt m)/(\sqrt L + \sqrt m)$ and $x_{-1} = x_0$.</li>
</ul>

<p>Draw a plot of the convergence behavior on the run, plotting the iteration number against $\log_{10}[f(x_k ) − f(x^* )]$. Use a single plot, with different colors for the different algorithms.</p>

<p>You can use the following code as a starting point:</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="c1"># Write the function f here</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="c1"># Write the fist order derivative here</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">GradientDescent</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="w"> </span><span class="n">x_star</span><span class="p">,</span><span class="w"> </span><span class="n">L</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1e-6</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">
  </span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span><span class="w">
  </span><span class="n">x1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x0</span><span class="w">
  </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x_star</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">e</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1"># Write your updates here:</span><span class="w">
    </span><span class="c1"># x1 = </span><span class="w">
    </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="w">
    </span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span><span class="w">
  </span><span class="p">}</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">Nesterov</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="w"> </span><span class="n">x_star</span><span class="p">,</span><span class="w"> </span><span class="n">L</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1e-6</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="c1"># Write your function here</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>

<p>Also, you can utilize the following codes for plotting:</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get optimization results:</span><span class="w">
</span><span class="n">out_GD_1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">GradientDescent</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="w"> </span><span class="n">x_star</span><span class="p">,</span><span class="w"> </span><span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="n">out_GD_2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">GradientDescent</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="w"> </span><span class="n">x_star</span><span class="p">,</span><span class="w"> </span><span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="n">out_Nest</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Nesterov</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="w"> </span><span class="n">x_star</span><span class="p">,</span><span class="w"> </span><span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="p">)</span><span class="w">
</span><span class="c1"># Plots:</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"n"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">500</span><span class="p">),</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-7</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">out_GD_1</span><span class="p">[[</span><span class="m">3</span><span class="p">]],</span><span class="w"> </span><span class="m">10</span><span class="p">))</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">out_GD_2</span><span class="p">[[</span><span class="m">3</span><span class="p">]],</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'blue'</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">out_Nest</span><span class="p">[[</span><span class="m">3</span><span class="p">]],</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'red'</span><span class="p">)</span><span class="w">
</span><span class="n">legend</span><span class="p">(</span><span class="s2">"topright"</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"GD1"</span><span class="p">,</span><span class="w"> </span><span class="s2">"GD2"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Nest"</span><span class="p">),</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">))</span><span class="w">

</span></code></pre></div></div>

<p>If you are interested in the convergence property of the Nesterov’s method, you can refer to page 32-40 of <a href="http://www.optimization-online.org/DB_FILE/2016/12/5748.pdf">this book</a>.</p>

<hr />

  </div>
</div>

<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true},
jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
TeX: {
extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
equationNumbers: {
autoNumber: "AMS"
}
}
});
</script>
      </div>
      <hr>
      <footer>
        <p>
          <!-- start of footer -->
          Fall 2020 &nbsp;-&nbsp;
          <a href="http://mlqmlq.github.io">Linquan Ma</a>
          <!-- end of footer -->
        </small></p>
      </footer>

    </div>

    
  </body>
</html>


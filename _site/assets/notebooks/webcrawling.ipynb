{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrawling\n",
    "\n",
    "- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "In the ipython you can use\n",
    "\n",
    "```ipython\n",
    "!pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple html\n",
    "\n",
    "Here is one example of html file, how to extract these item? Coffee, Tea, Coke\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Welcome to My Website</h1>\n",
    "        <ul>\n",
    "            <li>Coffee</li>\n",
    "            <li>Tea</li>\n",
    "            <li>Coke</li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Imagine we want to get the items in the list. The ul tag indicates an unordered list. We’ll then want to get each list item (list items are in li tags). Specifically, we’ll want to extract the text inside each list item. To do this, we’ll use the following code, where `example` is the HTML of the page.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "soup = BeautifulSoup(example, 'html.parser')\n",
    "items = soup.find(\"ul\").find_all(\"li\")\n",
    "```\n",
    "\n",
    "You’ll notice that items is a list of three items, since there are three list items in the unordered list. You’ll also see that items[0].text will give you the text of the first list item!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Welcome to My Website</h1>\n",
    "        <ul>\n",
    "            <li>Coffee</li>\n",
    "            <li test = 'heheda'>Tea</li>\n",
    "            <li favorate = 'hehe'>\n",
    "                <span>Coke</span>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(example, 'html.parser')\n",
    "items = soup.find(\"ul\").find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[2].find('span').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = []\n",
    "for item in items:\n",
    "    it.append(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course website scrapping\n",
    "\n",
    "| Time         | Food                                   |   Calorie |\n",
    "| :----------- | :------------------------------------- | --------: |\n",
    "| breakfast    | egg, milk, cereal, avocado             |       600 |\n",
    "| lunch        | chicken breast, brown rice, lettuce    |       700 |\n",
    "| dinner       | steak, sweet potato, broccoli          |       800 |\n",
    "\n",
    "\n",
    "The homepage [http://peigenzhou.com/stat628/pages/notes0308.html](http://peigenzhou.com/stat628/pages/notes0308.html) has a table which shows above, how do we get all the food and calorie from this table?\n",
    "\n",
    "The following are the source code for this table. Which can use google chrom `Inspect` or `View Page Source` to check\n",
    "\n",
    "```html\n",
    "<table rules=\"groups\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"text-align: left\">Time</th>\n",
    "      <th style=\"text-align: left\">Food</th>\n",
    "      <th style=\"text-align: right\">Calorie</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"text-align: left\">breakfast</td>\n",
    "      <td style=\"text-align: left\">egg, milk, cereal, avocado</td>\n",
    "      <td style=\"text-align: right\">600</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align: left\">lunch</td>\n",
    "      <td style=\"text-align: left\">chicken breast, brown rice, lettuce</td>\n",
    "      <td style=\"text-align: right\">700</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align: left\">dinner</td>\n",
    "      <td style=\"text-align: left\">steak, sweet potato, broccoli</td>\n",
    "      <td style=\"text-align: right\">800</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"text-align: left\"> </td>\n",
    "      <td style=\"text-align: left\"> </td>\n",
    "      <td style=\"text-align: right\"> </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "```\n",
    "\n",
    "We need to use `request` to ask python to look through the webpage, and `BeautifulSoup` to parse the html text for us.\n",
    "\n",
    "Sometimes we may see [404 page](http://peigenzhou.com/stat628/pages/notes0309.html), we can use `*.status_code` to check, here are the list of [status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://peigenzhou.com/stat628/pages/notes0308.html\"\n",
    "req_page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_page.status_code ## Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_page.content ## Raw contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = req_page.content\n",
    "page = BeautifulSoup(page_content, 'html.parser') ## Use beacutiful to parse the html, so that it will be easy to manipulate the webpage\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.find(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.find_all(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_part = page.find(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_part.find_all(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_part.find_all(\"td\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hard coding here... Very very bad practice\n",
    "food = table_part.find_all(\"td\")\n",
    "\n",
    "info = {}\n",
    "\n",
    "for i in range(3):\n",
    "    i = i * 3\n",
    "    info[food[i].text] = {\"Food\": food[i+1].text, \"Calorie\": food[i+2].text}\n",
    "\n",
    "print(\"Our table is: \", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "This is from [Brown cs1915A scraping](https://cs.brown.edu/courses/csci1951-a/assignments/scraping.html)\n",
    "\n",
    "To get started, we’re going to want to collect some data on the most active stocks in the market. Conveniently, Yahoo Finance [publishes this exact data](https://finance.yahoo.com/most-active). To collect this data, you’ll make use of web scraping.\n",
    "\n",
    "For purposes of this assignment, we've made a copy of this page to keep the data static. Note, some of the data in our static copy is intentionally modified from real stock data to ensure you've handled edge cases. As such, you will scrape from this URL: [https://cs.brown.edu/courses/csci1951-a/resources/yahoo_finance.html](https://cs.brown.edu/courses/csci1951-a/resources/yahoo_finance.html)\n",
    "\n",
    "Before scraping, you'll need your code to access this webpage. You should make use of the `request` library to make an HTTP request and collect the HTML. If you're not familar with the `request` library, you can read about it [here](http://docs.python-requests.org/en/master).\n",
    "\n",
    "Once you have accessed the HTML and assigned it to some variable, you'll want to scrape it, collecting the following for each stock in the table.\n",
    "\n",
    "* company name\n",
    "* price\n",
    "* market cap\n",
    "* percentage daily change\n",
    "\n",
    "You'll use Beautiful Soup, a Python package, to scrape the HTML. This will require looking at the HTML structure of the Yahoo Finance page. You can select various HTML elements on a page by tag name, class name, and/or id. Using [inspect element](https://zapier.com/blog/inspect-element-tutorial/) on your web browser, you can check what HTML tags and classes contain the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your code here\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Excerse\n",
    "\n",
    "Apple today [https://api.iextrading.com/1.0/stock/aapl/chart/1d](https://api.iextrading.com/1.0/stock/aapl/chart/1d)\n",
    "\n",
    "Rather than using web scraping to collect this data, we’ll make use of an API. You’ll make requests to this API using Python’s `request` library. IEX Trading offers an API with various endpoints that offer information about stocks. We’re going to want to collect two pieces of information for each stock in Yahoo’s most active stock table:\n",
    "\n",
    "* the average closing price of each of the most active stocks over the last 6 months\n",
    "* the number of articles recently written about each stock\n",
    "\n",
    "To do this, you’ll want to make use of the [chart endpoint](https://iextrading.com/developer/docs/#chart) to collect the historical stock pricing. Then, you will want to parse through the data and average the closing price for each day.\n",
    "\n",
    "Using the [news endpoint](https://iextrading.com/developer/docs/#news), you should get the articles for a specific stock. Then, you should count how many articles were returned by the API.\n",
    "\n",
    "**Hint**: Some stocks from Yahoo are not listed on major stock exchanges, and thus the IEX Trading API does not have data on them. In this case, the IEX Trading API will return a [404 status](https://en.wikipedia.org/wiki/HTTP_404) code. Your program should handle this error by disregarding stocks from Yahoo if they are not present in the IEX Trading API. That is, these stocks should not be added to the database. You can check the status code of a request by checking `requests.get(...).status_code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Your code here\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

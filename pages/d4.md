---
layout: page
title: Discussion 4
description: notes, links, example code, exercises
---
## Grading policy for Module 1 (coding portion)

Loss function:

``` r
loss <- function(t, x) {
  if (t < x) {
    return((t-x)^2)
  } 
  if(t <= x+10)
    return(1000)
  if(t <= x+20)
    return(5000)
  if(t <= x+30)
    return(10000)
  if(t <= x+40)
    return(20000)
  if(t <= x+50)
    return(40000)
  return((x/2)^2)
}
```

How did we generate the 9-dim vector in the report:

The first three elements indicates whether your code throws error for the three dataset we use. If not, you get 1. Otherwise, you get 0. If your code doesn't throw error for any of them, at least you will get 2 points for the coding portion.

The fourth through sixth elements are generated by 
```r
loss(your_function(y_t,t,maxcap), trueTime)
```

The last three elements represents the running time of your code, generated by the following code:
```r
codetime = rep(0,10)
for(i in 1:10) {
  start=Sys.time()
  output = your_function(y_t,t,maxcap) 
  end=Sys.time()
  codetime[i] = as.numeric(end-start)
}
```

You will get more points for  lower loss ($\le 10000$), quick running time (finishes within $10^{-3}s$) and good-looking scalability plot.

Also, the score for the coding portion only reflects how well your predictions are on those three **specific** datasets. Don't be upset if you didn't get a satisfied score, because

- The second and thrid worth much more points than the first one
- The grade distribution of this course based on the history is generous (nearly all the students got an A)

If you have any concerns for the grade of the coding portion, feel free to talk with me during my office hours.

## Nonlinear Opitmization

Most of the time, when training a machine learning model, we need to perform continuous nonlinear opttimization over an objective function. In this discussion, I will cover some basic optimizers and their properties:

- Gradient descent
- Nesterov's accelarated gradient descent
- Conjugate gradient
- Stochastic gradient descent

Throughout the discussion, we consider the unconstrained minimization of a smooth convex function:
\\[\min_{x\in\mathbb R^n} f(x)\\]

### Gradient descent



---
